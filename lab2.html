<!DOCTYPE html>

<!--
THEME MODIFIED FROM THEMEFISHER HTML TEMPLATE
 // WEBSITE: https://themefisher.com
-->

<html lang="zxx">

	<head>
		<meta charset="utf-8" />
		<meta
			name="viewport"
			content="width=device-width, initial-scale=1, shrink-to-fit=no"
		/>
		<meta name="author" content="Spike" />
		<!-- <link rel="icon" href="icon.gif" type="image/gif" /> -->
		<link rel="icon" href="img/me_circle.png" />
		<title>Fast Robots Lab 1</title>
		<meta name="keywords" content="lab1, fastRobots, arduino, artemis nano," />

  <!-- ** Plugins Needed for the Project ** -->
  <!-- Bootstrap -->
  <link rel="stylesheet" href="css/bootstrap.min.css">
  <!-- Main Stylesheet -->
  <link href="css/style.css" rel="stylesheet">
  <link href="css/page.css" rel="stylesheet">

</head>

<body>
  
<!-- Header Start --> 
<nav class="navbar navbar-expand-lg sticky-top navbar-dark bg-dark tab-navigation" style="z-index: 1020">
    <div class="container-xxl">
        <ul class="nav nav-tabs w-100" style="border-bottom: none">
            <li class="nav-item flex-fill text-center">
                <a href="index.html" class="nav-link tab-link">Home</a>
            </li>

            <li class="nav-item flex-fill text-center">
                <a href="#" class="nav-link tab-link active">Fast Robots</a>
            </li>
            
            <li class="nav-item flex-fill text-center">
                <a href="lab1.html" class="nav-link tab-link" style="font-size: 0.9em;">Lab 1</a>
            </li>
            <li class="nav-item flex-fill text-center">
                <a href="lab2.html" class="nav-link tab-link active" style="border-bottom: 2px solid #47daff; font-size: 0.9em;">Lab 2</a>
            </li>
            <li class="nav-item flex-fill text-center">
                <a href="lab3.html" class="nav-link tab-link" style="font-size: 0.9em;">Lab 3</a>
            </li>
        </ul>
    </div>
</nav>



<!-- Header Close --> 

<!-- Hero Section -->
<section class="section pt-5 pb-0">
  <div class="container">
    <div class="row">
      <div class="col-lg-4">
        <div class="mb-4">
		  <h2 class="mb-5">THIS AINT A THING YET</h2>
		  <img src="img/lab1Box.gif" alt="Fast Robots Lab 1" style="width: 100%; height: auto; display: block; border-radius: 8px; margin-bottom: 20px;">
          <p><span class="text-color-hover-light">Arduino IDE</span> • <span class="text-color-hover-light">Bluetooth</span> • <span class="text-color-hover-light">Jupyter Notebooks</span></p> 
		</div>
	</div>
      <div class="col-lg-8">
        <div class="mb-4">
          <h3 class="text-color mb-4">Introduction to Using Artemis Nano</h3>
          <p class="lead">This lab is to serve as an introduction to using the Arduino IDE and Jupyter notebooks  to communicate with the Artemis Nano microcontroller. We look at a few peripherals and communicate with the board via bluetooth and serial communication.</p>
          <!-- <p class="mt-3">The interaction is intentionally lightweight: the system records the user’s index-finger path only when the index finger is the only finger extended, and the user ends the trace with an “OK-sign.” From that captured point cloud, the pipeline reconstructs a clean polygonal outline and generates evenly spaced fence poses the robot can execute.</p> -->
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Hero Video -->
<!-- <section class="section pt-0 pb-5">
	<div class="container">
		<div class="row">
			<div class="col-12">
				<video controls autoplay muted loop style="width: 100%; height: auto; display: block; border-radius: 8px;" poster="images/portfolio/HRC/hrc2_cover_wide.png">
					<source src="images/portfolio/HRC/HRC_Codesign.mp4" type="video/mp4">
					<img src="images/portfolio/HRC/hrc2_cover_wide.png" alt="Human-Robot Co-design" style="width: 100%; height: auto; display: block; border-radius: 8px;">
				</video>
			</div>
		</div>
	</div>
</section> -->

<!-- Design Development Section -->
<section class="section pt-5 pb-5 bg-secondary">
  <div class="container">
    <div class="row mb-5">
      <div class="col-lg-8 offset-lg-2">
        <h4 class="text-color-hover-light mb-2">Research Approach</h4>
        <h2 class="mb-4">A Novel Interface for Co-creation</h2>
        <p>Working with the hrc² lab at Cornell, we developed an interactive platform that enables humans to communicate design intent through natural hand gestures, which the robot interprets and executes through precise manipulation.</p>
        <p class="mt-3">Designing a “language” of collaboration to feel like natural human communication rather than a robot UI. The system combines computer vision, gesture recognition, and robotic control to create a seamless feedback loop where both human and robot contribute their unique strengths to the creative process.</p>
      </div>
    </div>
  </div>
</section>

<!-- Engineering Section -->
<section class="section pt-5 pb-5">
  <div class="container">
    <div class="row mb-5">
      <div class="col-lg-8 offset-lg-2">
        <h4 class="text-color-hover-light mb-2">Technical Implementation</h4>
        <div>
			<h2 class="mb-4">Camera and sensing: choosing stability over convenience</h2>
			<p>A key early decision was how the system should see the workspace.</p>
			<p><span class="text-color-hover"><strong>Alternatives considered: </strong></span>We considered using Kinova's onboard camera for a cleaner setup. However, the camera's reference frame moves with the end effector, and the arm's positional error can reach ~1 cm—exactly the same scale as the placement tolerance target. It also forces a "home pose" overhead for full-table visibility, slowing the human workflow. </p>
			<p class="mt-3 mb-5"><span class="text-color-hover"><strong>Final choice:</strong></span> a fixed webcam mounted ~1 m above the table to capture the full workspace continuously, outside the arm's operating volume (stable reference frame, no extra robot motion overhead).</p>
		</div>

		<div>
			<h2 class="mb-4">Robust gesture recognition with normalized landmarks</h2>
			<p>Gesture recognition is powered by hand landmarks (21 keypoints per hand) and simple geometric tests—built for reliability rather than novelty.</p>
			<p><span class="text-color-hover"><strong>What made it robust: </strong></span>Instead of using absolute distances between fingertips (which breaks across different hand sizes and distances to camera), I normalized measurements by palm scale. This made the gesture classifier invariant to hand size and depth, improving consistency across users. </p>
			<p class="mt-3 mb-5"><span class="text-color-hover"><strong>Design intent:</strong></span> keep it fast and interpretable—so failures are diagnosable and latency stays below the interaction threshold (targeted under ~200 ms). </p>
		</div>

		<div>
			<h2 class="mb-4">From noisy finger paths to fence-ready geometry</h2>
			<p>The hardest “thinking” part of the system is turning a noisy trace into a clean outline that preserves corners and is constructible with physical fence segments.</p>
			<p><span class="text-color-hover"><strong>Hard constraint: </strong></span>every edge between consecutive vertices must exceed the 5 cm fence length, otherwise the robot would be asked to place segments that physically can’t fit. </p>
			<p class="mt-3"><span class="text-color-hover"><strong>Design exploration: </strong></span>I tested each algorithm family across many traces (convex, concave, and “messy” user shapes) and judged them on: vertex fidelity (especially concave corners), constructibility (edge length), and stability. <br></p>
			<p><span><strong>1. tight α-concave hull (α=0.1): </strong></span> bad vertex interpretation + too many short edges (not fence-constructible).<br>
				<img src="images/portfolio/HRC/fig1.png" alt="Human-Robot Co-design" style="width: 100%; height: auto; display: block; border-radius: 8px;">
			</p>
			<p><span><strong>2. loose α-concave hull (α=0.5): </strong></span> better on convex corners but still collapses concave features; still produces short edges without extra processing.<br>
				<img src="images/portfolio/HRC/fig2.png" alt="Human-Robot Co-design" style="width: 100%; height: auto; display: block; border-radius: 8px;"> 
			</p>
			<p><span><strong>3. Convex hull + distance smoothing: </strong></span> constructible edges, but convex hull fundamentally cannot represent concavity; also missed important convex vertices in some traces.<br>
				<img src="images/portfolio/HRC/fig3.png" alt="Human-Robot Co-design" style="width: 100%; height: auto; display: block; border-radius: 8px;"> 
			</p>
			<p class="mb-5"><span><strong>4. Ramer–Douglas–Peucker (RDP) + distance smoothing (final): </strong></span> handled both concave and convex shapes, preserved key vertices, and could be recursively tuned until edges satisfied the 5 cm constraint.<br>
				<img src="images/portfolio/HRC/fig4.png" alt="Human-Robot Co-design" style="width: 100%; height: auto; display: block; border-radius: 8px;"> 
			</p>
		</div>

		<div>
			<h2 class="mb-4">Aligning vision space to robot space</h2>
			<p>The full system only works if the CV output becomes a valid robot input.</p>
			<p class="mt-3">I integrated the modules by converting the camera-derived trace coordinates into the robot’s reference frame using transformation matrices and scaling, so the planned fence poses match the Kinova control interface. </p>
			
		</div>
		</div>	
    </div>
    

<!-- Research Findings Section -->
<section class="section pt-5 pb-5 bg-secondary">
  <div class="container">
    <div class="row mb-5">
      <div class="col-lg-8 offset-lg-2">
        <h4 class="text-color-hover-light mb-2">Research Outcomes</h4>
        <h2 class="mb-4">Insights on Human-Robot Collaboration</h2>
        <p>The research revealed key insights into how humans and robots can effectively collaborate on creative tasks. Participants found the gesture-based interface intuitive and appreciated the robot's ability to maintain precision while they focused on conceptual design.</p>
        <p class="mt-3">The study demonstrated that combining human creativity with robotic precision creates opportunities for new forms of collaborative design work, with implications for manufacturing, education, and creative industries.</p>
      </div>
    </div>
  </div>
</section>

<!-- Results Section -->
<section class="section pt-5 pb-5">
  <div class="container">
    <div class="row">
      <div class="col-lg-8 offset-lg-2">
        <h4 class="text-color-hover-light mb-2">Impact & Future Work</h4>
        <h2 class="mb-4">Extending from “execute my sketch” to real co-creation</h2>
        <p>This research contributes to the growing field of human-robot collaboration by demonstrating practical methods for co-design. The findings have implications for future interactive systems where humans and robots work together as creative partners.</p>
        <p class="mt-3">Next steps include:</p>
		<p><span class="text-color-hover"><strong>User Studies </strong></span>with designers/planners to refine gestures, feedback, and failure recovery. </p>
		<p><span class="text-color-hover"><strong>Object Recognition </strong></span>on the table so the robot can reason about existing elements, not just place along a new outline. </p>
		<p><span class="text-color-hover"><strong>Design suggestion features </strong></span>to compute-and-suggest alternatives (spacing, accessibility, circulation) while keeping the human in control. </p>
		
			
        <div class="mt-5 p-4" style="border-left: 4px solid #47ffc5; background: #f7f7f7;">
          <p class="mb-0" style="font-style: italic;">"This work explores an exciting new paradigm where robots become active collaborators in the creative process, not just tools."</p>
          <p class="mt-2 mb-0 text-sm"><span class="text-color-hover">hrc² Lab, Cornell University</span></p>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Footer start -->
<section class="footer">
	<div class="container">
		<div class="row ">
			<div class="col-lg-6">
				<p class="mb-0">Copyrights © 2019. Designed & Developed by <a href="themefisher.com">Themefisher</a></p>
			</div>
			<div class="col-lg-6">
				<div class="widget footer-widget text-lg-right mt-5 mt-lg-0">
					<ul class="list-inline mb-0">
						<li class="list-inline-item"><a href="https://github.com/heisen-kong" target="_blank"><i class="ti-github mr-3"></i></a></li>
						<li class="list-inline-item"><a href="https://www.linkedin.com/in/heisen-kong/" target="_blank"><i class="ti-linkedin mr-3"></i></a></li>
						<li class="list-inline-item"><a href="https://www.instagram.com/heisens.creations/" target="_blank"><i class="ti-instagram mr-3"></i></a></li>
					</ul>
				</div>
			</div>
		</div>
	</div>
</section>
<!-- Footer End -->

<!-- jQuery -->
<script src="plugins/jQuery/jquery.min.js"></script>
<!-- Bootstrap JS -->
<script src="plugins/js/bootstrap.min.js"></script>
<script src="plugins/aos/aos.js"></script>
<script src="plugins/owl-carousel/owl.carousel.min.js"></script>
<script src="plugins/shuffle/shuffle.min.js"></script>
<script src="plugins/magnific-popup/jquery.magnific-popup.min.js"></script>
<script src="plugins/animated-text/animated-text.js"></script>
<script src="plugins/counto/apear.js"></script>
<script src="plugins/counto/counTo.js"></script>

 <!-- Google Map -->
<script src="plugins/google-map/map.js"></script>
<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyAkeLMlsiwzp6b3Gnaxd86lvakimwGA6UA&callback=initMap"></script> 
<!-- Main Script -->
<script src="js/script.js"></script>

</html>